{
 "cells": [
  {
   "source": [
    "<h1>Install & Import all necessary components and libraries </h1>\n",
    "<h3>Before execute this cell you need do this :</h3>\n",
    "<ul>\n",
    "    <li>cmd> python -m venv venv</li>\n",
    "    <li>cmd> venv/Scripts/activate</li>\n",
    "    <li>(venv)cmd> pip install -r requirements.txt</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cosine as dcos\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dropout, Activation, Permute\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "K.set_image_data_format( 'channels_last' )"
   ]
  },
  {
   "source": [
    "<h2>Creation of neural network matrix</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convblock(cdim, nb, bits=3):\n",
    "    # Declaration of table \n",
    "    L = []\n",
    "    # Browse through all the bits of our image\n",
    "    for k in range(1,bits+1):\n",
    "        convname = 'conv'+str(nb)+'_'+str(k)\n",
    "        L.append(Convolution2D(cdim, kernel_size=(3, 3), padding='same', activation='relu', name=convname))\n",
    "    L.append(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_face_blank():\n",
    "    withDO = True # no effect during evaluation but usefull for fine-tuning\n",
    "    if True:\n",
    "        mdl = Sequential()\n",
    "        mdl.add( Permute((1,2,3), input_shape=(224,224,3)) )\n",
    "        for l in convblock(64, 1, bits=2):\n",
    "            mdl.add(l)\n",
    "        for l in convblock(128, 2, bits=2):\n",
    "            mdl.add(l)        \n",
    "        for l in convblock(256, 3, bits=3):\n",
    "            mdl.add(l)            \n",
    "        for l in convblock(512, 4, bits=3):\n",
    "            mdl.add(l)            \n",
    "        for l in convblock(512, 5, bits=3):\n",
    "            mdl.add(l)  \n",
    "                  \n",
    "        mdl.add( Convolution2D(4096, kernel_size=(7, 7), activation='relu', name='fc6') )\n",
    "        if withDO:\n",
    "            mdl.add( Dropout(0.5) )\n",
    "\n",
    "        mdl.add( Convolution2D(4096, kernel_size=(1, 1), activation='relu', name='fc7') )\n",
    "        if withDO:\n",
    "            mdl.add( Dropout(0.5) )\n",
    "\n",
    "        mdl.add( Convolution2D(2622, kernel_size=(1, 1), activation='relu', name='fc8') )\n",
    "        mdl.add( Flatten() )\n",
    "        mdl.add( Activation('softmax') )\n",
    "        return mdl\n",
    "    \n",
    "    else:\n",
    "        raise ValueError('not implemented')"
   ]
  },
  {
   "source": [
    "<h2>Importation of neural weight & bias</h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_mat_to_keras(kmodel):\n",
    "    kerasnames = [lr.name for lr in kmodel.layers]\n",
    "    prmt = (0,1,2,3)\n",
    " \n",
    "    for i in range(l.shape[1]):\n",
    "        matname = l[0,i][0,0].name[0]\n",
    "        if matname in kerasnames:\n",
    "            kindex = kerasnames.index(matname)\n",
    "            l_weights = l[0,i][0,0].weights[0,0]\n",
    "            l_bias = l[0,i][0,0].weights[0,1]\n",
    "            f_l_weights = l_weights.transpose(prmt)\n",
    "            assert (f_l_weights.shape == kmodel.layers[kindex].get_weights()[0].shape)\n",
    "            assert (l_bias.shape[1] == 1)\n",
    "            assert (l_bias[:,0].shape == kmodel.layers[kindex].get_weights()[1].shape)\n",
    "            assert (len(kmodel.layers[kindex].get_weights()) == 2)\n",
    "            kmodel.layers[kindex].set_weights([f_l_weights, l_bias[:,0]])"
   ]
  },
  {
   "source": [
    "<h2>Functions execution</h2> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model initialization\n",
    "facemodel = vgg_face_blank()\n",
    "\n",
    "# Load the pretrained weights into the model\n",
    "data = loadmat('vgg-face.mat', matlab_compatible=False, struct_as_record=False)\n",
    "l = data['layers']\n",
    "description = data['meta'][0,0].classes[0,0].description\n",
    "\n",
    "# Importation of neural weight & bias with facemodel \n",
    "copy_mat_to_keras(facemodel)\n",
    "\n",
    "# Final model that can get inputs and generate a prediction as an output\n",
    "featuremodel = Model( inputs = facemodel.layers[0].input, outputs = facemodel.layers[-2].output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a number, not '_io.TextIOWrapper'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1ca080f11136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_img-final.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvector_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mfeaturemodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jerem\\Bureau\\Faice\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\preprocessing\\image.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    230\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'dtype'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jerem\\Bureau\\Faice\\venv\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mimg_to_array\u001b[1;34m(img, data_format, dtype)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;31m# or (channel, height, width)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[1;31m# but original PIL image has format (width, height, channel)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'channels_first'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\jerem\\Bureau\\Faice\\venv\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not '_io.TextIOWrapper'"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "image = load_img(\"saved_img-final.jpg\")\n",
    "vector_image = img_to_array(image)\n",
    "# featuremodel.predict(vector_image)[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "d3259331a73204e8435feec00421330e1f63d2df065f8fab8d27759d18e1a48c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}